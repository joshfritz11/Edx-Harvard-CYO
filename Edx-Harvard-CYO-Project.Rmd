---
title: 'Understanding the Benefits and Limitations of the *Caret* Package using the UCI Dry Bean Dataset'
author: "Josh Fritz"
date: "2023-03-05"
always_allow_html: true
output:
  html_document: default
  word_document: default
  pdf_document:
    latex_engine: pdflatex
bibliography: references.bib
biblio-style: apalike
link-citations: yes
colorlinks: yes
urlcolor: blue
graphics: yes
header-includes:
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \usepackage{arev}
- \usepackage[T1]{fontenc}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
- \usepackage{fvextra}
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.ops=list(width.cutoff=60), tidy=TRUE)
```

## Introduction
The R programming language has several packages implementing machine learning. For linear regression alone, there are *logicFS*, *lars*, and *monomvn* to name a few [@kuhn]. Consequentially each package has different functions for training models and making predictions. To simplify these differences, the *caret* package has standardized training and predicting functions. Although *caret* standardizes machine learning functions, this simplification also prevents users from specifying parameters present in the algorithm's original package. For example, for training a multi-layer perceptron (MLP) model, the *caret* package has one tuning parameter `size` for specifying the number of units in the hidden layer. To contrast, the same algorithm initialized from the *RSNNS* package has several tuning parameters [@mlp]. The goal of this project was to better understand the benefits and limitations of the *caret* package using the UCI Dry Bean Dataset.

The Dry Bean Dataset was developed by Koklu and Ozkan at Selcuk University in Turkey [@koklu2020multiclass]. This dataset was chosen because it has over ten-thousand entries and Koklu and Ozkan implemented four machine learning algorithms for classifying the beans. The algorthims were: multi-layer perceptron (MLP), support vector machine (SVM), k-nearest neighbors (kNN), and decision tree (DT). Because there are published results for different algorithms on this dataset, this project's aim was to implement similar algorithms using the *caret* package and compare the results with the Koklu and Ozkan results.

#### *Dry Bean Dataset Description*
```{r Dry Bean Data Download, message=FALSE, warning=FALSE, include=FALSE}
if (!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if (!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# Dry Bean Dataset
# https://archive-beta.ics.uci.edu/dataset/602/dry+bean+dataset
# https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip

options(timeout = 120)

dl <- "DryBeanDataset.zip"
if (!file.exists(dl))
    download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip", dl)

Dry_Bean_Dataset_file <- "DryBeanDataset/Dry_Bean_Dataset.xlsx"
if (!file.exists(Dry_Bean_Dataset_file))
    unzip(dl, Dry_Bean_Dataset_file)

Dry_Bean_Dataset <- readxl::read_xlsx(Dry_Bean_Dataset_file,
          col_names = TRUE,
          trim_ws = TRUE)

# Test set will be 10% of the data.
set.seed(1, sample.kind = "Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = Dry_Bean_Dataset$Class, times = 1, p = 0.1, list = FALSE)
train_set <- Dry_Bean_Dataset[-test_index, ]
test_set <- Dry_Bean_Dataset[test_index, ]
```
The Dry Bean Dataset contains 13,611 entries and classifies the entries into one of seven different dry bean types. The seven dry bean types are: barbunya, bombay, cali, dermason, horoz, seker, and sira. Table 1 shows the observation count for the different types.

```{r Table 1: First five rows of MovieLens data table, message=FALSE, warning=FALSE, echo=FALSE}
if (!require(kableExtra)) install.packages("kableExtra")

library(kableExtra)

Bean_Type_Count <- Dry_Bean_Dataset %>%
    group_by(.$Class) %>%
    summarize(n())

kable_styling(kbl(Bean_Type_Count, booktabs = T, col.names = c("Bean Type", "Count")),
              latex_options = c("striped", "scale_down", "hold_position")) |>
  footnote(general = "Table 1: Count of bean types", general_title = "")
```

In addition to classifications, it has information about each dry bean's entry in the following variables:

* Area (A): The area of the bean zone and the number of pixels within its boundaries.
* Perimeter (P): Length of the bean's border.
* Major axis length (L): The distance of the longest line that can be drawn from a bean.
* Minor axis length (l): The longest line that can be drawn that is perpendicular to the main axis.
* Aspect ratio (k): The relationship between L and I as $K = \frac{L}{l}$.
* Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.
* Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.
* Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area. Written as $d = \sqrt{\frac{4 * A}{\pi}}$.
* Extent (Ex): The ratio of the pixels in the bounding box to the bean area. Written as $Ex = \frac{A}{A_B}$ where $A_B$ is the area of the bounding rectangle.
* Solidity (S): The ratio of the pixels in the convex shell to those found in beans. Written as $S = \frac{A}{C}$.
* Roundness (R): Calculated using $R = \frac{4 \pi A}{p^2}$.
* Compactness(CO): Measures the roundness of an object as $CO = \frac{Ed}{L}$.
* Shape Factor 1 (SF1): Calculated as $SF1 = \frac{L}{A}$.
* Shape Factor 2 (SF2): Calculated as $SF2 = \frac{l}{A}$.
* Shape Factor 3 (SF3): Calculated as $SF3 = \frac{A}{\frac{L}{2} * \frac{L}{2} * \pi}$.
* Shape Factor 4 (SF4): Calculated as $SF4 = \frac{A}{\frac{l}{2} * \frac{l}{2} * \pi}$.

## Methods and Analysis - explains the process and techniques used, including data cleaning, data exploration and visualization, any insights gained, and your modeling approach. At least two different models or algorithms must be used, with at least one being more advanced than linear or logistic regression for prediction problems.
Before training models and using them to make predictions, box plots for each variable were made to understand how the bean types differ from each other. From the box plots, it is apparent the Bombay type is distinct from the others with variables like area, perimeter, major axis length, minor axis length, convex area, and equivalent diameter. To contrast, the other bean types had regions of feature overlap with at least one other type, meaning it is difficult to differentiate them with descriptive statistics alone. Figure 1 shows example box plots for variables where Bombay beans are distinct and not distinct from the other types. Also note how other bean types are difficult to classify with these variables.
```{r Figure 1, message=FALSE, warning=FALSE, echo=FALSE, fig.show="hold", fig.alt="Figure 1: Box plots showing where Bombay beans are distinct and where they are not", fig.cap="Figure 1: Box plots showing where Bombay beans are distinct and where they are not"}
if (!require(gridExtra)) install.packages("gridExtra")
library(gridExtra)

AXIS_TEXT_X_ANGLE <- 45
AXIS_TEXT_VJUST <- 0.5

BoxPlot_Area <- Dry_Bean_Dataset |> ggplot(aes(Class, Area)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Area")

BoxPlot_Perimeter <- Dry_Bean_Dataset |> ggplot(aes(Class, Perimeter)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Perimeter")

BoxPlot_MajorAxisLength <- Dry_Bean_Dataset |> ggplot(aes(Class, MajorAxisLength)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Major Axis Length")

BoxPlot_MinorAxisLength <- Dry_Bean_Dataset |> ggplot(aes(Class, MinorAxisLength)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Minor Axis Length")

BoxPlot_AspectRatio <- Dry_Bean_Dataset |> ggplot(aes(Class, AspectRation)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Aspect Ratio")

BoxPlot_Eccentricity <- Dry_Bean_Dataset |> ggplot(aes(Class, Eccentricity)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Eccentricity")

BoxPlot_ConvexArea <- Dry_Bean_Dataset |> ggplot(aes(Class, ConvexArea)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Convex Area")

BoxPlot_EquivaDiameter <- Dry_Bean_Dataset |> ggplot(aes(Class, EquivDiameter)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Equivalent Diameter")

BoxPlot_Extent <- Dry_Bean_Dataset |> ggplot(aes(Class, Extent)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Extent")

BoxPlot_Solidity <- Dry_Bean_Dataset |> ggplot(aes(Class, Solidity)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Solidity")

BoxPlot_Roundness <- Dry_Bean_Dataset |> ggplot(aes(Class, roundness)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Roundness")

BoxPlot_Compactness <- Dry_Bean_Dataset |> ggplot(aes(Class, Compactness)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Compactness")

BoxPlot_ShapeFactor1 <- Dry_Bean_Dataset |> ggplot(aes(Class, ShapeFactor1)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Shape Factor 1")

BoxPlot_ShapeFactor2 <- Dry_Bean_Dataset |> ggplot(aes(Class, ShapeFactor2)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Shape Factor 2")

BoxPlot_ShapeFactor3 <- Dry_Bean_Dataset |> ggplot(aes(Class, ShapeFactor3)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Shape Factor 3")

BoxPlot_ShapeFactor4 <- Dry_Bean_Dataset |> ggplot(aes(Class, ShapeFactor4)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Shape Factor 4")

# Commented out this code because the output does not look good. However, can be uncommented for viewing as needed.
# Grob_Layout <- rbind(c(1, 2, 3, 4),
#                      c(5, 6, 7, 8),
#                      c(9, 10, 11, 12),
#                      c(13, 14, 15, 16))

# Grob_Plots <- arrangeGrob(grobs = list(BoxPlot_Area, BoxPlot_Perimeter, BoxPlot_MajorAxisLength, BoxPlot_MinorAxisLength,
#                                        BoxPlot_AspectRatio, BoxPlot_Eccentricity, BoxPlot_ConvexArea, BoxPlot_EquivaDiameter,
#                                        BoxPlot_Extent, BoxPlot_Solidity, BoxPlot_Roundness, BoxPlot_Compactness,
#                                        BoxPlot_ShapeFactor1, BoxPlot_ShapeFactor2, BoxPlot_ShapeFactor3, BoxPlot_ShapeFactor4), layout_matrix = Grob_Layout)

Grob_Layout <- rbind(c(1, 2),
                     c(3, 4))

Grob_Plots <- arrangeGrob(grobs = list(BoxPlot_Area, BoxPlot_Perimeter,
                                       BoxPlot_Extent, BoxPlot_Solidity), layout_matrix = Grob_Layout)
grid.arrange(Grob_Plots)
```

For training and testing models, the Dry Bean Dataset was partitioned into training and test sets using *caret's* `createDataPartition` function. The partition was made with 90% of the data going to the training set while 10% went to the test set. These proportions were chosen because they are similar to the ones used in Koklu and Ozkan paper [@koklu2020multiclass]. However, in their study, they used 10-fold cross-validation instead of partitioning separate training and test sets. This means for every iteration in cross-validation used 90% of the data for training and 10% for testing.
```{r Dry Bean Training and Test Set Partitioning, message=FALSE, warning=FALSE, include=FALSE}
# Test set will be 10% of the data.
set.seed(1, sample.kind = "Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = Dry_Bean_Dataset$Class, times = 1, p = 0.1, list = FALSE)
train_set <- Dry_Bean_Dataset[-test_index, ]
test_set <- Dry_Bean_Dataset[test_index, ]
```

The first models created were control models to understand the accuracy of guessing bean types. The models were a random guessing model and a proportional guessing model. The random guessing model randomly assigns a bean type for every row in the test set using R's `sample` function. Although random guesses produce a worst-case model, they do not reflect the proportion of bean types in the training set as shown in figure 2. Therefore the proportional model was made using the training set to reflect its bean type proportions. The proportion model was also made with the `sample` by setting its `prob` parameter to the training set's proportion of bean types. Figure 3 shows how the proportion model's output compares to the proportion of the training set's bean types.
```{r Control Models, message=FALSE, warning=FALSE, include=FALSE}
# Random guessing model
bean_classes <- unique(train_set$Class)
test_set_length <- length(test_set$Class)

y_hat_random <- tibble(Class = sample(bean_classes, test_set_length, replace = TRUE))

# Proportional guessing model
bean_proportions <- map_df(bean_classes, function(bean_class){
    list(Class = bean_class,
         Proportion = mean(train_set$Class == bean_class))
})

y_hat_proportion <- tibble(Class = sample(bean_classes, test_set_length, replace = TRUE, prob = bean_proportions$Proportion))
```

```{r Figure 2, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.show="hold", out.width="49%", out.height="49%", fig.alt="Figure 2: Comparing distributions between random guessing model and training set. The distributions are not similar.", fig.cap="Figure 2: Comparing distributions between random guessing model and training set. The distributions are not similar."}
CAPTION_FONT_SIZE <- 12

y_hat_random |> ggplot(aes(Class)) + 
  geom_bar(color = "black") +
  theme_light() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.caption = element_text(hjust = 0, size = CAPTION_FONT_SIZE)) + 
  labs(x = "", y = "Count",
       caption = "a: Random prediction bean types have uniform distribution")

train_set |> ggplot(aes(Class)) + 
  geom_bar(color = "black") +
  theme_light() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.caption = element_text(hjust = 0, size = CAPTION_FONT_SIZE)) + 
  labs(x = "", y = "Count",
       caption = "b: Training set bean types do not have a uniform distribution")
```
```{r Figure 3, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.show="hold", out.width="49%", out.height="49%", fig.alt="Figure 3: Comparing distributions between proportional guessing model and train set. The distributions are similar.", fig.cap="Figure 3: Comparing distributions between proportional guessing model and train set. The distributions are similar."}
y_hat_proportion |> ggplot(aes(Class)) + 
  geom_bar(color = "black") +
  theme_light() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.caption = element_text(hjust = 0, size = CAPTION_FONT_SIZE)) + 
  labs(x = "", y = "Count",
       caption = "a: Proportional guessing model")

train_set |> ggplot(aes(Class)) + 
  geom_bar(color = "black") +
  theme_light() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.caption = element_text(hjust = 0, size = CAPTION_FONT_SIZE)) + 
  labs(x = "", y = "Count",
       caption = "b: Training set bean types")
```
TODO: Create the models from the Koklu and Ozkan paper. Reference the Machine Learning textbook where possible.
TODO: Describe the differences in implementing the Caret model versus the Koklu and Ozkan models.

The first machine learning model created was a multi-layer perceptron model. This model is an artificial neural network that is robust to training errors and can be applied to discrete-valued problems [@mitchell2007machine]. It takes inspiration from biology through systems of interconnected neurons. Instead of neurons, it creates an interconnected set of simple units that each take several inputs and produce a single output.

```{r Multi-layer Perceptron Models, message=FALSE, warning=FALSE, include=FALSE}
# MLP without pre-processing
control <- trainControl(method = "cv", number = 10)

train_mlp <- train(Class ~ .,
                   data = train_set,
                   method = "mlp",
                   tuneGrid = data.frame(size = seq(1, 5, 1)),
                   trControl = control)

y_hat_mlp <- predict(train_mlp, test_set)

# MLP with pre-processing
train_mlp_preprocessing <- train(Class ~ .,
                                 data = train_set,
                                 method = "mlp",
                                 tuneGrid = data.frame(size = seq(1, 11, 2)),
                                 trControl = control,
                                 preProcess = c("center", "scale"))

y_hat_mlp_preprocessing <- predict(train_mlp_preprocessing, test_set)
```

## Results - presents the modeling results and discusses the model performance.
TODO: Display each model's accuracy.
TODO: Discuss how the model accuracies differed between the Koklu and Ozkan and my models.

## Conclusion - gives a brief summary of the report, its potential impact, its limitations, and future work.


## References {.unnumbered}