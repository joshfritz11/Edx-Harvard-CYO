---
title: 'Understanding the Benefits and Limitations of the *Caret* Package using the UCI Dry Bean Dataset'
author: "Josh Fritz"
date: "2023-03-12"
always_allow_html: true
output:
  html_document: default
  pdf_document:
    latex_engine: pdflatex
  word_document: default
bibliography: references.bib
biblio-style: apalike
link-citations: yes
colorlinks: yes
urlcolor: blue
graphics: yes
header-includes:
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \usepackage{arev}
- \usepackage[T1]{fontenc}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
- \usepackage{fvextra}
- \usepackage[singlelinecheck=false]{caption}
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.ops=list(width.cutoff=60), tidy=TRUE)
```

## Introduction
The R programming language has several packages implementing machine learning. For linear regression alone, there are *logicFS*, *lars*, *monomvn*, and others [@kuhn]. Consequentially each package has distinct functions for training models and making predictions. To simplify these distinctions, the *caret* package has standard training and predicting functions. Although *caret* standardizes machine learning functions, this simplification also prevents users from specifying parameters present in the algorithm's original package. For example, for training a multi-layer perceptron model, the *caret* package has one tuning parameter, `size`, for specifying the number of units in the hidden layer. To contrast, the same algorithm initialized from the *RSNNS* package has several tuning parameters [@mlp]. The goal of this project was to better understand the benefits and limitations of the *caret* package using the Dry Bean Dataset from the University of California Irvine.

The Dry Bean Dataset was developed by Murat Koklu and Ilker Ali Ozkan at Selcuk University in Turkey [@koklu2020multiclass]. This dataset was chosen because it has over ten-thousand entries and Koklu and Ozkan's study implemented four machine learning algorithms for classifying the beans. The algorthims were multi-layer perceptron, support vector machine, k-nearest neighbors, and decision tree. Because there are published results for different algorithms on this dataset, this project's aim was to implement similar algorithms using the *caret* package and compare the results.

#### *Dry Bean Dataset Description*
```{r Dry Bean Data Download, message=FALSE, warning=FALSE, include=FALSE}
if (!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if (!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# Dry Bean Dataset
# https://archive-beta.ics.uci.edu/dataset/602/dry+bean+dataset
# https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip

options(timeout = 120)

dl <- "DryBeanDataset.zip"
if (!file.exists(dl))
    download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip", dl)

Dry_Bean_Dataset_file <- "DryBeanDataset/Dry_Bean_Dataset.xlsx"
if (!file.exists(Dry_Bean_Dataset_file))
    unzip(dl, Dry_Bean_Dataset_file)

Dry_Bean_Dataset <- readxl::read_xlsx(Dry_Bean_Dataset_file,
          col_names = TRUE,
          trim_ws = TRUE)

# Test set will be 10% of the data.
set.seed(1, sample.kind = "Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = Dry_Bean_Dataset$Class, times = 1, p = 0.1, list = FALSE)
train_set <- Dry_Bean_Dataset[-test_index, ]
test_set <- Dry_Bean_Dataset[test_index, ]
```
The Dry Bean Dataset contains 13,611 entries and classifies the entries into one of seven dry bean types. The dry bean types are barbunya, bombay, cali, dermason, horoz, seker, and sira. Table 1 shows the observation count for the distinct types.

```{r Table 1: Count of bean types, message=FALSE, warning=FALSE, echo=FALSE}
if (!require(kableExtra)) install.packages("kableExtra")

library(kableExtra)

Bean_Type_Count <- Dry_Bean_Dataset %>%
    group_by(.$Class) %>%
    summarize(n())

kable_styling(kbl(Bean_Type_Count, booktabs = T, col.names = c("Bean Type", "Count")),
              latex_options = c("striped", "hold_position")) |>
  footnote(general = "Table 1: Count of bean types", general_title = "")
```

In addition to classifications, it has information about each dry bean's characteristics in the following variables:

* Area (A): The area of the bean zone and the number of pixels within its boundaries.
* Perimeter (P): Length of the bean's border.
* Major axis length (L): The distance of the longest line that can be drawn from a bean.
* Minor axis length (l): The longest line that can be drawn that is perpendicular to the main axis.
* Aspect ratio (k): The relationship between L and I as $K = \frac{L}{l}$.
* Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.
* Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.
* Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area. Written as $d = \sqrt{\frac{4 * A}{\pi}}$.
* Extent (Ex): The ratio of the pixels in the bounding box to the bean area. Written as $Ex = \frac{A}{A_B}$ where $A_B$ is the area of the bounding rectangle.
* Solidity (S): The ratio of the pixels in the convex shell to those found in beans. Written as $S = \frac{A}{C}$.
* Roundness (R): Calculated using $R = \frac{4 \pi A}{p^2}$.
* Compactness (CO): Measures the roundness of an object as $CO = \frac{Ed}{L}$.
* Shape Factor 1 (SF1): Calculated as $SF1 = \frac{L}{A}$.
* Shape Factor 2 (SF2): Calculated as $SF2 = \frac{l}{A}$.
* Shape Factor 3 (SF3): Calculated as $SF3 = \frac{A}{\frac{L}{2} * \frac{L}{2} * \pi}$.
* Shape Factor 4 (SF4): Calculated as $SF4 = \frac{A}{\frac{l}{2} * \frac{l}{2} * \pi}$.

## Methods and Analysis
#### *Data Visualization*
Before training models and using them to make predictions, box plots for each variable were made to understand how the bean types differ from each other. From the box plots, it is apparent the Bombay type is distinct from the others with variables like area, perimeter, major axis length, minor axis length, convex area, and equivalent diameter. To contrast, the other bean types had regions of feature overlap with at least one other type, meaning it is difficult to differentiate them with descriptive statistics alone. Figure 1 shows example box plots for variables where Bombay beans are distinct and not distinct from the other types. Also note how other bean types are difficult to classify with these variables.
```{r Figure 1, message=FALSE, warning=FALSE, echo=FALSE, fig.show="hold", fig.alt="Figure 1: Box plots showing where Bombay beans are distinct and where they are not", fig.cap="Figure 1: Box plots showing where Bombay beans are distinct and where they are not"}
if (!require(gridExtra)) install.packages("gridExtra")
library(gridExtra)

AXIS_TEXT_X_ANGLE <- 45
AXIS_TEXT_VJUST <- 0.5

BoxPlot_Area <- Dry_Bean_Dataset |> ggplot(aes(Class, Area)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Area")

BoxPlot_Perimeter <- Dry_Bean_Dataset |> ggplot(aes(Class, Perimeter)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Perimeter")

BoxPlot_MajorAxisLength <- Dry_Bean_Dataset |> ggplot(aes(Class, MajorAxisLength)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Major Axis Length")

BoxPlot_MinorAxisLength <- Dry_Bean_Dataset |> ggplot(aes(Class, MinorAxisLength)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Minor Axis Length")

BoxPlot_AspectRatio <- Dry_Bean_Dataset |> ggplot(aes(Class, AspectRation)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Aspect Ratio")

BoxPlot_Eccentricity <- Dry_Bean_Dataset |> ggplot(aes(Class, Eccentricity)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Eccentricity")

BoxPlot_ConvexArea <- Dry_Bean_Dataset |> ggplot(aes(Class, ConvexArea)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Convex Area")

BoxPlot_EquivaDiameter <- Dry_Bean_Dataset |> ggplot(aes(Class, EquivDiameter)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Equivalent Diameter")

BoxPlot_Extent <- Dry_Bean_Dataset |> ggplot(aes(Class, Extent)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Extent")

BoxPlot_Solidity <- Dry_Bean_Dataset |> ggplot(aes(Class, Solidity)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Solidity")

BoxPlot_Roundness <- Dry_Bean_Dataset |> ggplot(aes(Class, roundness)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Roundness")

BoxPlot_Compactness <- Dry_Bean_Dataset |> ggplot(aes(Class, Compactness)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Compactness")

BoxPlot_ShapeFactor1 <- Dry_Bean_Dataset |> ggplot(aes(Class, ShapeFactor1)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Shape Factor 1")

BoxPlot_ShapeFactor2 <- Dry_Bean_Dataset |> ggplot(aes(Class, ShapeFactor2)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Shape Factor 2")

BoxPlot_ShapeFactor3 <- Dry_Bean_Dataset |> ggplot(aes(Class, ShapeFactor3)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Shape Factor 3")

BoxPlot_ShapeFactor4 <- Dry_Bean_Dataset |> ggplot(aes(Class, ShapeFactor4)) +
    geom_boxplot() +
    theme_light() + 
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          axis.text.x = element_text(angle = AXIS_TEXT_X_ANGLE, vjust = AXIS_TEXT_VJUST),
          panel.grid.minor.y = element_blank()) +
    labs(x = "", y = "Shape Factor 4")

# Commented out this code because the output does not look good. However, can be uncommented for viewing as needed.
# Grob_Layout <- rbind(c(1, 2, 3, 4),
#                      c(5, 6, 7, 8),
#                      c(9, 10, 11, 12),
#                      c(13, 14, 15, 16))

# Grob_Plots <- arrangeGrob(grobs = list(BoxPlot_Area, BoxPlot_Perimeter, BoxPlot_MajorAxisLength, BoxPlot_MinorAxisLength,
#                                        BoxPlot_AspectRatio, BoxPlot_Eccentricity, BoxPlot_ConvexArea, BoxPlot_EquivaDiameter,
#                                        BoxPlot_Extent, BoxPlot_Solidity, BoxPlot_Roundness, BoxPlot_Compactness,
#                                        BoxPlot_ShapeFactor1, BoxPlot_ShapeFactor2, BoxPlot_ShapeFactor3, BoxPlot_ShapeFactor4), layout_matrix = Grob_Layout)

Grob_Layout <- rbind(c(1, 2),
                     c(3, 4))

Grob_Plots <- arrangeGrob(grobs = list(BoxPlot_Area, BoxPlot_Perimeter,
                                       BoxPlot_Extent, BoxPlot_Solidity), layout_matrix = Grob_Layout)
grid.arrange(Grob_Plots)
```
#### *Data Partitioning*
For training and testing models, the Dry Bean Dataset was partitioned into training and test sets using *caret's* `createDataPartition` function. The partition was made with 90% of the data going to the training set while 10% went to the test set. These proportions were chosen because they are like the ones used in Koklu and Ozkan study [@koklu2020multiclass]. However, in their study they used 10-fold cross-validation instead of partitioning separate training and test sets. This means for every iteration in cross-validation used 90% of the data for training and 10% for testing.
```{r Dry Bean Training and Test Set Partitioning, message=FALSE, warning=FALSE, include=FALSE}
# Test set will be 10% of the data.
set.seed(1, sample.kind = "Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = Dry_Bean_Dataset$Class, times = 1, p = 0.1, list = FALSE)
train_set <- Dry_Bean_Dataset[-test_index, ]
test_set <- Dry_Bean_Dataset[test_index, ]
```

#### *Control Models*
The first models created were control models to understand the accuracy of guessing bean types. The models were a random guessing model and a proportional guessing model. The random guessing model randomly assigns a bean type for every row in the test set using R's `sample` function. Although random guesses produce a worst-case model, they do not reflect the proportion of bean types in the training set as shown in figure 2. Therefore, the proportional model was made using the training set to reflect its bean type proportions. The proportion model was also made with the `sample` by setting its `prob` parameter to the training set's proportion of bean types. Figure 3 shows how the proportion model's output compares to the proportion of the training set's bean types.
```{r Control Models, message=FALSE, warning=FALSE, include=FALSE}
# Random guessing model
bean_classes <- unique(train_set$Class)
test_set_length <- length(test_set$Class)

y_hat_random <- tibble(Class = sample(bean_classes, test_set_length, replace = TRUE))

# Proportional guessing model
bean_proportions <- map_df(bean_classes, function(bean_class){
    list(Class = bean_class,
         Proportion = mean(train_set$Class == bean_class))
})

y_hat_proportion <- tibble(Class = sample(bean_classes, test_set_length, replace = TRUE, prob = bean_proportions$Proportion))
```

```{r Figure 2, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.show="hold", out.width="49%", out.height="49%", fig.alt="Figure 2: Comparing distributions between random guessing model and training set. The distributions are not similar.", fig.cap="Figure 2: Comparing distributions between random guessing model and training set. The distributions are not similar."}
CAPTION_FONT_SIZE <- 12

y_hat_random |> ggplot(aes(Class)) + 
  geom_bar(color = "black") +
  theme_light() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.caption = element_text(hjust = 0, size = CAPTION_FONT_SIZE)) + 
  labs(x = "", y = "Count",
       caption = "a: Random prediction bean types have uniform distribution")

train_set |> ggplot(aes(Class)) + 
  geom_bar(color = "black") +
  theme_light() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.caption = element_text(hjust = 0, size = CAPTION_FONT_SIZE)) + 
  labs(x = "", y = "Count",
       caption = "b: Training set bean types do not have a uniform distribution")
```
```{r Figure 3, message=FALSE, warning=FALSE, echo=FALSE, fig.align="center", fig.show="hold", out.width="49%", out.height="49%", fig.alt="Figure 3: Comparing distributions between proportional guessing model and train set. The distributions are similar.", fig.cap="Figure 3: Comparing distributions between proportional guessing model and train set. The distributions are similar."}
y_hat_proportion |> ggplot(aes(Class)) + 
  geom_bar(color = "black") +
  theme_light() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.caption = element_text(hjust = 0, size = CAPTION_FONT_SIZE)) + 
  labs(x = "", y = "Count",
       caption = "a: Proportional guessing model")

train_set |> ggplot(aes(Class)) + 
  geom_bar(color = "black") +
  theme_light() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.caption = element_text(hjust = 0, size = CAPTION_FONT_SIZE)) + 
  labs(x = "", y = "Count",
       caption = "b: Training set bean types")
```
#### *Multi-layer Perceptron Model*
The first machine learning model created was a multi-layer perceptron model. This model is an artificial neural network that is robust to training errors and can be applied to discrete-valued problems [@mitchellmachine]. It takes inspiration from biology through systems of interconnected neurons. However, instead of neurons it creates an interconnected set of simple units that each take several inputs and produce a single output.

Training the multi-layer perceptron model was done using the *caret* package's `train` function using the training set. The function was set to learn the class using all the dry bean's variables. The method used was "mlp" from the *RSNNS* package [@mlp]. To improve the model's performance, 10-fold cross-validation was used like the Koklu and Ozkan study. Although cross-validation was used there were several parameters that differed from Koklu and Ozkan's study. For example, Koklu and Ozkan specified a hidden layer number of two, while the *caret* model was allowed to set its hidden layer number based on the results of the train function. To do this, the model's `size` parameter was set to try hidden layer numbers from one to five. These numbers were chosen based on the model's results. For example, if the model had a size of five (i.e., the maximum number selected), then the range of sizes would have been increased until a size within the range was selected. Additionally, Koklu and Ozkan's study specified other parameters like hidden layer activation function, learning rate, and performance goal. The *caret* model did not have options for setting these parameters, but the function does set these parameters when it is executed.

After training the first multi-layer perceptron model, the model's accuracy was lower than expected. To increase the model's accuracy, a second model was made using the *caret* `preProcess` parameter. This parameter was set to center and scale the data before it was used to make predictions. The results of this model are discussed in the results section.

The code for both models is shown below:
```{r Multi-layer Perceptron Models, message=FALSE, warning=FALSE}
if (!require(RSNNS)) install.packages("RSNNS")

# MLP without pre-processing
control <- trainControl(method = "cv", number = 10)

train_mlp <- caret::train(Class ~ .,
                   data = train_set,
                   method = "mlp",
                   tuneGrid = data.frame(size = seq(1, 5, 1)),
                   trControl = control)

y_hat_mlp <- predict(train_mlp, test_set)

# MLP with pre-processing
train_mlp_preprocessing <- caret::train(Class ~ .,
                                 data = train_set,
                                 method = "mlp",
                                 tuneGrid = data.frame(size = seq(1, 11, 2)),
                                 trControl = control,
                                 preProcess = c("center", "scale"))

y_hat_mlp_preprocessing <- predict(train_mlp_preprocessing, test_set)
```


#### *Support Vector Machine Model*
The second model was made with a support vector machine. Building this model was done like the multi-layer perceptron model using the *caret* `train` function on the training data with 10-fold cross-validation. Unlike the multi-layer perceptron model, the method used was "svmLinear" from the *kernlab* package [@svm]. Furthermore, the tuning parameter specified was cost, which was set to try values from 35 to 40. Specifying this parameter differed from the Koklu and Ozkan study because they did not specify cost. Instead, they specified what type of generalization to use, which was one-vs.-one. Finally, the `preProcess` parameter was set to center and scale the data to improve accuracy.

This model's code is shown below:
```{r Support Vector Machine Model, message=FALSE, warning=FALSE}
control <- trainControl(method = "cv", number = 10)

train_svm <- caret::train(Class ~ .,
                   data = train_set,
                   method = "svmLinear",
                   tuneGrid = data.frame(C = seq(35, 40, 1)),
                   trControl = control,
                   preProcess = c("center", "scale"))

y_hat_svm <- predict(train_svm, test_set)
```


#### *k-Nearest Neighbors Model*
The third model was made using k-nearest neighbors. This method uses instance-based learning, which stores the training examples in memory without generalizing beyond them [@mitchellmachine]. Generalization occurs when a new instance must be classified. Building this model was like the other *caret* models. The method used was "knn" from the *caret* package [@kuhn]. In this case, the tuning parameter was the number of neighbors, which was set to try values between 10 and 15. In the Koklu and Ozkan study, the number of neighbors specified was thirteen.

This model's code is shown below:
```{r k-Nearest Neighbors Model, message=FALSE, warning=FALSE}
control <- trainControl(method = "cv", number = 10)

train_knn <- caret::train(Class ~ .,
                   data = train_set,
                   method = "knn",
                   tuneGrid = data.frame(k = seq(10, 15, 1)),
                   trControl = control,
                   preProcess = c("center", "scale"))

y_hat_knn <- predict(train_knn, test_set)
```


#### *Random Forest Model*
The fourth model made is also the last model to compare with the Koklu and Ozkan study. The Koklu and Ozkan study used a decision tree model, but for this study the *caret* model chosen was a random forest from the *randomForest* package [@randomForest]. Random forest was used because it improves predictions by averaging multiple decision trees [@irizarry2022]. For the random forest model, several tuning parameters were specified. First, the number of variables randomly sampled was 1, 5, 10, 25, 50, and 100. Next the number of trees was set to 150. Finally, the size of samples to draw was five thousand. To contrast from this study's model, the Koklu and Ozkan study's decision tree model specified a maximum number of partitions as sixteen. This parameter could not be specified in the random forest model.

This model's code is shown below:
```{r Random Forest Model, message=FALSE, warning=FALSE}
if (!require(randomForest)) install.packages("randomForest")

control <- trainControl(method = "cv", number = 10)
grid <- data.frame(mtry = c(1, 5, 10, 25, 50, 100))

train_rf <- caret::train(Class ~ .,
                  data = train_set,
                  method = "rf",
                  ntree = 150,
                  trControl = control,
                  tuneGrid = grid,
                  nsamp = 5000,
                  preProcess = c("center", "scale"))

y_hat_rf <- predict(train_rf, test_set)
```


#### *Naive Bayes Classifier Model*
The final model made was a naive Bayes classifier. This model was not used in the Koklu and Ozkan study, but was chosen because its performance is competitive with algorithms like decision trees and neural networks [@mitchellmachine]. For this study, the method "naive_bayes" was used from the *naivebayes* package [@naivebayes]. Like the study's other models, 10-fold cross-validation was used, but no tuning parameters were set. Also like the other models, the `preProcess` parameter was set to center and scale the data.

This model's code is shown below:
```{r Naive Bayes Classifier Model, message=FALSE, warning=FALSE}
if (!require(naivebayes)) install.packages("naivebayes")

control <- trainControl(method = "cv", number = 10)

train_naive_bayes_classifier <- caret::train(Class ~ .,
                                      data = train_set,
                                      method = "naive_bayes",
                                      trControl = control,
                                      preProcess = c("center", "scale"))

y_hat_naive_bayes_classifier <- predict(train_naive_bayes_classifier, test_set)
```


## Results
```{r Results of all models, message=FALSE, warning=FALSE, include=FALSE}
# Build tibble of results for Koklu and Ozkan study and this study.
model_accuracy_results <- tibble(Method = "Random Model", "Koklu and Ozkan Study Accuracy (%)" = "N/A", 
                                 "Caret Model Accuracy (%)" = caret::confusionMatrix(factor(y_hat_random$Class), factor(test_set$Class))$overall[["Accuracy"]] * 100)
model_accuracy_results <- model_accuracy_results |> add_row(Method = "Proportion Model", "Koklu and Ozkan Study Accuracy (%)" = "N/A",
                                                            "Caret Model Accuracy (%)" = caret::confusionMatrix(factor(y_hat_proportion$Class), factor(test_set$Class))$overall[["Accuracy"]] * 100)
model_accuracy_results <- model_accuracy_results |> add_row(Method = "Multi-layer Perceptron", "Koklu and Ozkan Study Accuracy (%)" = "91.73", 
                                                            "Caret Model Accuracy (%)" = caret::confusionMatrix(y_hat_mlp, factor(test_set$Class))$overall[["Accuracy"]] * 100)
model_accuracy_results <- model_accuracy_results |> add_row(Method = "Multi-layer Perceptron + Pre-processing", "Koklu and Ozkan Study Accuracy (%)" = "91.73",
                                                            "Caret Model Accuracy (%)" = caret::confusionMatrix(y_hat_mlp_preprocessing, factor(test_set$Class))$overall[["Accuracy"]] * 100)
model_accuracy_results <- model_accuracy_results |> add_row(Method = "Support Vector Machine", "Koklu and Ozkan Study Accuracy (%)" = "93.13", 
                                                            "Caret Model Accuracy (%)" = caret::confusionMatrix(y_hat_svm, factor(test_set$Class))$overall[["Accuracy"]] * 100)
model_accuracy_results <- model_accuracy_results |> add_row(Method = "knn", "Koklu and Ozkan Study Accuracy (%)" = "92.52",
                                                            "Caret Model Accuracy (%)" = caret::confusionMatrix(y_hat_knn, factor(test_set$Class))$overall[["Accuracy"]] * 100)
model_accuracy_results <- model_accuracy_results |> add_row(Method = "Random Forest",  "Koklu and Ozkan Study Accuracy (%)" = "87.92",
                                                            "Caret Model Accuracy (%)" = caret::confusionMatrix(y_hat_rf, factor(test_set$Class))$overall[["Accuracy"]] * 100)
model_accuracy_results <- model_accuracy_results |> add_row(Method = "Naive Bayes", "Koklu and Ozkan Study Accuracy (%)" = "N/A",
                                                            "Caret Model Accuracy (%)" = caret::confusionMatrix(y_hat_naive_bayes_classifier, factor(test_set$Class))$overall[["Accuracy"]] * 100)
```
Each model's accuracy was calculated using the *caret* package's `confusionMatrix` function. Table 2 compares each model's accuracy results between the Koklu and Ozkan models and the *caret* models.

```{r Table 2: Model accuracy results, message=FALSE, warning=FALSE, echo=FALSE}
kable_styling(kbl(model_accuracy_results, booktabs = T, digits = 2, align = "lll"),
              latex_options = c("striped", "hold_position", "scale_down")) |>
  footnote(general = "Table 2: Model accuracy results", general_title = "")
```

All the models had higher accuracies than the control models, which is expected. Most of the Koklu and Ozkan models had better accuracies than the *caret* models with the exceptions of the multi-layer perceptron with pre-processing and random forest models. The random forest model may have performed better because the algorithm created several decision trees and averaged the results, while the Koklu and Ozkan study only used a single decision tree. For the naive Bayes model, this model did not perform as well as any of the Koklu and Ozkan models, however it was only off from the other models by less than five percent. This is interesting because the Bayes model is not as sophisticated as the other machine learning models and performed almost as well as them.

For the multi-layer perceptron model without pre-processing, the model's accuracy was much lower than expected. Table 3 shows this model's confusion matrix. From the confusion matrix, it is clear the model predicted Dermason for every item in the test set, but it is unclear why this model chose Dermason while the multi-layer perceptron model with pre-processing did not.
```{r Table 3: Multi-layer perceptron confusion matrix, message=FALSE, warning=FALSE, echo=FALSE}
mlp_table <- caret::confusionMatrix(y_hat_mlp, factor(test_set$Class))$table
kable_styling(kbl(mlp_table, booktabs = T, digits = 2)) |>
  footnote(general = "Table 3: Multi-layer perceptron confusion matrix", general_title = "")
```
## Conclusion
The R programming language has several packages implementing machine learning and each package has distinct functions for training and predicting. In this study, the *caret* package was used to understand its benefits and limitations compared to the various machine learning packages. The benefits of using the *caret* package include using the same functions for training models and making predictions, automatically finding the best model by setting tuning parameters' ranges, and using functions like `confusionMatrix` to evaluate each model's performance. To contrast, one limitation of the *caret* package includes not being able to set all tuning parameters offered by the original machine learning package. Despite this limitation, the *caret* package offers tools for prototyping and comparing several machine learning models quickly. This means a model could be chosen among many, and then implemented using the original machine learning package and fine-tuning the model's parameters. Further study is needed to understand how fine-tuning a model compares with using the *caret* package alone. Furthermore, it is not known how a *caret* model performance compares with non-caret models when larger datasets are used.

## References {.unnumbered}