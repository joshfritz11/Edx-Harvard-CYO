---
title: 'Understanding the Benefits and Limitations of the *Caret* Package using the UCI Dry Bean Dataset'
author: "Josh Fritz"
date: "2023-03-05"
always_allow_html: true
output:
  pdf_document:
    latex_engine: pdflatex
  word_document: default
  html_document: default
bibliography: references.bib
biblio-style: apalike
link-citations: yes
colorlinks: yes
urlcolor: blue
graphics: yes
header-includes:
- \usepackage{caption}
- \captionsetup[figure]{labelformat=empty}
- \usepackage{arev}
- \usepackage[T1]{fontenc}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
- \usepackage{fvextra}
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.ops=list(width.cutoff=60), tidy=TRUE)
```

## Introduction
The R programming language has several packages implementing machine learning. For linear regression alone, there are *logicFS*, *lars*, and *monomvn* to name a few [@kuhn]. Consequentially each package has different functions for training models and making predictions. To simplify these differences, the *caret* package has standardized training and predicting functions. Although *caret* standardizes machine learning functions, this simplification also prevents users from specifying parameters present in the algorithm's original package. For example, for training a multi-layer perceptron (MLP) model, the *caret* package has one tuning parameter `size` for specifying the number of units in the hidden layer. To contrast, the same algorithm initialized from the *RSNNS* package has several tuning parameters [@mlp]. The goal of this project was to better understand the benefits and limitations of the *caret* package using the UCI Dry Bean Dataset.

The Dry Bean Dataset was developed by Koklu and Ozkan at Selcuk University in Turkey [@koklu2020multiclass]. This dataset was chosen because it has over ten-thousand entries and Koklu and Ozkan implemented four machine learning algorithms for classifying the beans. The algorthims were: multi-layer perceptron (MLP), support vector machine (SVM), k-nearest neighbors (kNN), and decision tree (DT). Because there are published results for different algorithms on this dataset, this project's aim was to implement similar algorithms using the *caret* package and compare the results with the Koklu and Ozkan results.

#### *Dry Bean Dataset Description*
The Dry Bean Dataset contains 13,611 entries and classifies the entries into one of seven different dry bean types. The seven dry bean types are: barbunya, bombay, cali, dermason, horoz, seker, and sira.

In addition to classifications, it has information about each dry bean's entry in the following variables:

* Area (A): The area of the bean zone and the number of pixels within its boundaries.
* Perimeter (P): Length of the bean's border.
* Major axis length (L): The distance of the longest line that can be drawn from a bean.
* Minor axis length (l): The longest line that can be drawn that is perpendicular to the main axis.
* Aspect ratio (k): The relationship between L and I as $K = \frac{L}{l}$.
* Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.
* Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.
* Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area. Written as $d = \sqrt{\frac{4 * A}{\pi}}$.
* Extent (Ex): The ratio of the pixels in the bounding box to the bean area. Written as $Ex = \frac{A}{A_B}$ where $A_B$ is the area of the bounding rectangle.
* Solidity (S): The ratio of the pixels in the convex shell to those found in beans. Written as $S = \frac{A}{C}$.
* Roundness (R): Calculated using $R = \frac{4 \pi A}{p^2}$.
* Compactness(CO): Measures the roundness of an object as $CO = \frac{Ed}{L}$.
* Shape Factor 1 (SF1): Calculated as $SF1 = \frac{L}{A}$.
* Shape Factor 2 (SF2): Calculated as $SF2 = \frac{l}{A}$.
* Shape Factor 3 (SF3): Calculated as $SF3 = \frac{A}{\frac{L}{2} * \frac{L}{2} * \pi}$.
* Shape Factor 4 (SF4): Calculated as $SF4 = \frac{A}{\frac{l}{2} * \frac{l}{2} * \pi}$.

## Methods and Analysis - explains the process and techniques used, including data cleaning, data exploration and visualization, any insights gained, and your modeling approach. At least two different models or algorithms must be used, with at least one being more advanced than linear or logistic regression for prediction problems.
```{r Dry Bean Data Setup, message=FALSE, warning=FALSE, include=FALSE}
if (!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if (!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# Dry Bean Dataset
# https://archive-beta.ics.uci.edu/dataset/602/dry+bean+dataset
# https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip

options(timeout = 120)

dl <- "DryBeanDataset.zip"
if (!file.exists(dl))
    download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip", dl)

Dry_Bean_Dataset_file <- "DryBeanDataset/Dry_Bean_Dataset.xlsx"
if (!file.exists(Dry_Bean_Dataset_file))
    unzip(dl, Dry_Bean_Dataset_file)

Dry_Bean_Dataset <- readxl::read_xlsx(Dry_Bean_Dataset_file,
          col_names = TRUE,
          trim_ws = TRUE)

# Test set will be 10% of the data.
set.seed(1, sample.kind = "Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = Dry_Bean_Dataset$Class, times = 1, p = 0.1, list = FALSE)
train_set <- Dry_Bean_Dataset[-test_index, ]
test_set <- Dry_Bean_Dataset[test_index, ]
```



## Results - presents the modeling results and discusses the model performance.


## Conclusion - gives a brief summary of the report, its potential impact, its limitations, and future work.


## References {.unnumbered}